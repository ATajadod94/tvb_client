{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/Ali/Desktop/Baycrest/virtualbrain'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/akacollja/tvb_scientific/tvb-library')\n",
    "sys.path.append('/home/akacollja/tvb_scientific/tvb-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "\n",
    "import sys\n",
    "nbso,nbse = sys.stdout,sys.stderr # hack part 1/2 to keep output printing properly\n",
    "\n",
    "from tvb.simulator.lab import *\n",
    "#LOG= get_logger('demo')\n",
    "from tvb.simulator.plot.tools import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.stdout,sys.stderr = nbso,nbse  # ...hack part 2/2\n",
    "#from tvb.simulator.plot.tools import plot_surface_mpl,plot_surface_mpl_mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nilearn.plotting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9a0d05dff6e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnibabel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnilearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_roi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_affine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nilearn.plotting"
     ]
    }
   ],
   "source": [
    "import os, sys, glob,h5py, itertools, multiprocessing\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from scipy import optimize\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image,display as d\n",
    "import seaborn as sns\n",
    "\n",
    "import nibabel as nib\n",
    "from nilearn.plotting import plot_roi\n",
    "from dipy.tracking.utils import apply_affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab nbagg\n",
    "from tvb.simulator.lab import *\n",
    "from tvb.simulator.plot.phase_plane_interactive import PhasePlaneInteractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab nbagg\n",
    "from tvb.simulator.lab import *\n",
    "LOG= get_logger('demo')\n",
    "from tvb.simulator.plot.tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring information flow between memory and oculomotor systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look into possible pathways and their functional significance. We will start off by running a V1 stimulations to check our parameters. Once you are happy with your activation times and parameters. We will stimulation one of the hippocampal nodes and check their activation times if they ever activate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the stimulus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by choosing a specific node to stimulate, in this case V1, and defining the weighting of the stimulus coming into that node. Below we'll define the temporal profile with the default Gaussian equation. \n",
    "\n",
    "We ran the simulation for 14000 ms and onset our stimulus at 10000ms to wait for the transient to decay.\n",
    "\n",
    "Period ('T'): pulse repetition period is set at 5000\n",
    "Tau: pulse width or pulse duration is set at 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1_node = 68\n",
    "stim_name= 'V1'\n",
    "#spatial pattern\n",
    "weighting = numpy.zeros((154,))\n",
    "weighting [[v1_node]] = 0.03\n",
    "\n",
    "#temporal profile\n",
    "eqn_t = equations.PulseTrain()\n",
    "eqn_t.parameters['onset'] = 10000\n",
    "eqn_t.parameters['T'] = 2001.0\n",
    "eqn_t.parameters['tau'] = 50.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a connectivity matrix that is not the default matrix and so we will need to load in this matrix. Make sure after loading it in you should name the files appropriately, zip them and then place the package in the connectivity file in tvb's data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new_conn= connectivity.Connectivity.from_file(\"newconn.zip\")\n",
    "#new_conn.configure()\n",
    "#new_conn.weights= (np.loadtxt('/home/htian/Data/avg9/weights.txt'))\n",
    "#new_conn.tract_lengths= np.loadtxt('/home/htian/Data/avg9/tract_lengths.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'newconn.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-288c2b83abd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_conn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mconnectivity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectivity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"newconn.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#new_conn.weights= (np.loadtxt('/home/akacollja/Data/avg9/weights.txt'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/akacollja/Data/avg9/weightsLaplaceEquation.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#new_conn.weights= (np.loadtxt('/home/akacollja/Data/avg9/newWeightsCA3.txt'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ali/miniconda3/envs/py27/lib/python2.7/site-packages/tvb/datatypes/connectivity.pyc\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(source_file, instance)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_full_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_array_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ali/miniconda3/envs/py27/lib/python2.7/site-packages/tvb/basic/readers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, zip_path)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip_archive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Ali/miniconda3/envs/py27/lib/python2.7/zipfile.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0mmodeDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'r'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'r+b'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'newconn.zip'"
     ]
    }
   ],
   "source": [
    "new_conn= connectivity.Connectivity.from_file(\"newconn.zip\")\n",
    "new_conn.configure()\n",
    "#new_conn.weights= (np.loadtxt('/home/akacollja/Data/avg9/weights.txt'))\n",
    "new_conn.weights= (np.loadtxt('/home/akacollja/Data/avg9/weightsLaplaceEquation.txt'))\n",
    "#new_conn.weights= (np.loadtxt('/home/akacollja/Data/avg9/newWeightsCA3.txt'))\n",
    "new_conn.tract_lengths= np.loadtxt('/home/akacollja/Data/avg9/tract_lengths.txt')\n",
    "#new_conn.tract_lengths= np.loadtxt('/home/akacollja/tract_lengths.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, combine the spatial pattern with the temporal profile into one object. We must first configure the stimulus which happens automatically in the simulator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stimulus = patterns.StimuliRegion( temporal = eqn_t, \n",
    "                                  connectivity = new_conn, \n",
    "                                  weight = weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stimulus.configure_space()\n",
    "stimulus.configure_time(numpy.arange(0.,11500, 2**-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must define the parameters for our simulation. Here we are using parameters that we've come up with based off of the Spiegler model and previous trials. Adjust these values to what best fits your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Coupling: The coupling function is applied to the activity propagated between regions by the \"Long-range connectivity\" before it enters the local dynamic equations of the Model. Its primary purpose is to 'rescale' the incoming activity to a level appropriate to Model.\n",
    "\n",
    "2. Integrator: \"A tvb.simulator.Integrator object which is an integration scheme with supporting attributes such as integration step size and noise specification for stochastic methods. It is used to compute the time courses of the model state variables.\n",
    "\n",
    "3. Monitors: A tvb.simulator.Monitor or a list of tvb.simulator.Monitor objects that 'know' how to record relevant data from the simulation. Two main types exist: 1) simple, spatial and temporal, reductions (subsets or averages); 2) physiological measurements, such as EEG, MEG and fMRI. By default the Model's specified variables_of_interest are returned, temporally downsampled from the raw integration rate to a sample rate of 1024Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model= models.Generic2dOscillator(d = (1/1000.) * 76.74,\n",
    "                                tau =1., f= 1.,e= 0., g= -0.1, \n",
    "                            alpha =1., gamma= 1., c= 0., b= -12.3083, beta =0., a =0.)\n",
    "heunint = integrators.HeunDeterministic(dt = 0.1) \n",
    "#integrators = integrators.HeunDeterministic(dt = 0.2)\n",
    "#integrators = integrators.HeunStochastic(dt = 0.1, noise=noise.Additive(nsig=5e-15)) \n",
    "#integrators = integrators.HeunStochastic(dt = 0.1, noise=noise.Additive(nsig=0)) \n",
    "cpl = coupling.Linear(a=0.01)\n",
    "#cpl = coupling.Linear(a=0.01)\n",
    "mons= monitors.TemporalAverage(period = 1.0)\n",
    "new_conn.speed=5\n",
    "#new_conduction.speed= conduction.speed(conduction.speed=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conduction_speed=new_conn.speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Phase Plane Interactive tool to explore the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PhasePlaneInteractive??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppi_fig = PhasePlaneInteractive(model=model, integrator=heunint)\n",
    "ppi_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and run the simulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. conduction_speed: Conduction speed for \"Long-range connectivity\" (mm/ms)\n",
    "2. stimulus: a \"Spatiotemporal stimulus\" can be defined at the region or surface level. It's composed of spatial and temporal components. For region defined stimuli the spatial component is just the strength with which the temporal component is applied to each region. For surface defined stimuli,  a (spatial) function, with finite-support, is used to define the strength of the stimuli on the surface centred around one or more focal points. In the current version of TVB, stimuli are applied to the first state variable of the 'Local dynamic model'.\n",
    "3. initial_conditions: Initial conditions from which the simulation will begin. By default, random initial conditions are provided. Needs to be the same shape as simulator 'history', ie, initial history function which defines the minimal initial state of the network with time delays before time t=0. n_time, n_svar, n_node, n_mode <--> time points, state variables, nodes, modes\n",
    "4. time points:             determined by the integration time step and simulation length\n",
    "5. state-variables (sv): determined by the population model \n",
    "6. nodes:                    determined by the regions connectivity matrix\n",
    "7. modes:                   determined by the population model \n",
    "8. simulation_length: The length of a simulation in milliseconds (ms).\n",
    "9. connectivity: A tvb.datatypes.Connectivity object which contains the structural long-range connectivity data (i.e., white-matter tracts). In combination with the 'Long-range coupling function' it defines the inter-regional connections. These couplings undergo a time delay via signal propagation with a propagation speed of \"Conduction Speed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#current_state = arrays.FloatArray(required=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import tvb.datatypes.arrays as arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sim.current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#numpy.zeros(154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sim.state_variable_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for (i,n_time) in n_time,\n",
    "\n",
    "#result= [n_time, n_svar, n_node, n_mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#n_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new_conn.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = 0.1*np.ones(sim.good_history_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sim = simulator.Simulator(\n",
    "        model = model,\n",
    "        connectivity = new_conn,\n",
    "        initial_conditions = init,\n",
    "        coupling = cpl,\n",
    "        integrator = heunint,\n",
    "        monitors = mons,\n",
    "        stimulus = stimulus, \n",
    "        simulation_length = 11500,\n",
    "        #conduction_speed = 3.0\n",
    "        #conduction_speed = new_conn.speed\n",
    "        ).configure()\n",
    "\n",
    "\n",
    "#n_time, n_svar, n_node, n_mode = sim.good_history_shape\n",
    "#initial = np.empty((n_time,n_svar, n_node, n_mode)) \n",
    "\n",
    "#for (n_time, n_svar),\n",
    "\n",
    "#for i in n_time:\n",
    "#for n_time in range(sim.good_history_shape)\n",
    "#result[0,n_time,n_svar,n_node,n_mode]= correlate()\n",
    "#for i = n_time:\n",
    "\n",
    "    #initial[0,:,:,:][0,:,0] =0.1*np.ones((154,))\n",
    "    #initial[0,:,:,:][1,:,0] =0.1*np.ones((154,))\n",
    "    #initial[1,:,:,:][0,:,0] =0.1*np.ones((154,))\n",
    "    #initial[1,:,:,:][1,:,0] =0.1*np.ones((154,))\n",
    "\n",
    "\n",
    "#sim.initial_conditions = init\n",
    "\n",
    "new_conn.configure\n",
    "\n",
    "sim.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#init.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(tavg_time, tavg_data), = sim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#new_conn = connectivity.Connectivity(load_default=True) # (or whatever you normally put here)\n",
    "#new_conn.speed = 8.0\n",
    "#new_conn.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the stimulated node's amplitude is much greater than the amplitudes of the other nodes. To be able to see the data in an appropriate scope we will exclude the stimulated node from certain calculations. Here we are inlcuding all the data except the data of the stimulated node into an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst_nodes = [i for i in range(154) if i != v1_node]\n",
    "nostimnode=tavg_data[:, 0, lst_nodes, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph, the black lines will represent all nodes except the stimulated node, red is for the stimulated node, and green is for certain nodes that we want to investigate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas_raw= (nostimnode)\n",
    "v1_raw= (tavg_data[:, 0, v1_node, 0])\n",
    "follow_raw=( tavg_data[:, 0, 36, 0],tavg_data[:, 0, 45, 0],tavg_data[:, 0, 46, 0],tavg_data[:, 0, 47, 0],\n",
    "             tavg_data[:, 0, 69, 0],tavg_data[:, 0, 70, 0],tavg_data[:, 0, 72, 0])\n",
    "#follow nodes are in the order FEF, MSTd, MSTl, MT, V1, V2, V3, V4 \n",
    "\n",
    "datas= np.squeeze(datas_raw)\n",
    "v1= np.squeeze(v1_raw).T\n",
    "follow = np.squeeze(follow_raw).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#time vs temporal average graph\n",
    "figure()\n",
    "plot(tavg_time, datas,'k', alpha = 0.5)\n",
    "#plot(tavg_time, v1, 'r', alpha = 1)\n",
    "plot(tavg_time, follow, 'g', alpha =0.5)\n",
    "axes=plt.gca()\n",
    "axes.set_ylim([-0.15, 0.15])\n",
    "ylabel(\"Temporal average\")\n",
    "xlabel (\"Time(ms)\")\n",
    "plt.grid('off')\n",
    "\n",
    "#savefig('/home/akacollja/tvb/fig_dir/timeseries_V1', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled timeseries for V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#time vs temporal average graph\n",
    "figure()\n",
    "plot(tavg_time, datas,'k', alpha = 0.5)\n",
    "#plot(tavg_time, v1, 'red', alpha = 1)\n",
    "plot(tavg_time, follow, 'g', alpha =0.5)\n",
    "axes=plt.gca()\n",
    "axes.set_xlim([9975,12000]) #([9950,10700])\n",
    "axes.set_ylim([-0.02, 0.02])\n",
    "ylabel(\"Temporal average\")\n",
    "xlabel (\"Time(ms)\")\n",
    "plt.grid('off')\n",
    "\n",
    "#savefig('/home/akacollja/tvb/fig_dir/timeseries_V1scaled', bbox_inches='tight')\n",
    "\n",
    "#savefig('/hestia/ryan_lab/byang/Byang/deterministic/timeseries_V1scaled.png', bbox_inches='tight')\n",
    "#savefig('/home/byang/fig_dir/timeseries_V1scaled', bbox_inches='tight')\n",
    "\n",
    "\n",
    "# #axes.get_xlim[10000,11000]\n",
    "# \n",
    "# get_xlim(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding thresholds\n",
    "\n",
    "We will need to define thresholds of each invidual node. Here we are using the data 200 ms before stimlation. After finding the maximum amplitude of each node in that time period, we will make an array in the same shape as the shape of the data we are comparing it to. Next, look for when the data is greater than the maximum then when it is less than the maximum. Finally, we add them together in a single array to see when it passes threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_notrans = tavg_time[range(9800,12000)]\n",
    "data_notrans_mean= tavg_data.squeeze().T[:,range(9800, 10000)]\n",
    "data_notrans= tavg_data.squeeze().T[:,range(9800, 12000)]\n",
    "\n",
    "data_notrans_posthr= []\n",
    "data_notrans_negthr= []\n",
    "data_notrans_thr= []\n",
    "\n",
    "for node in range(len(data_notrans_mean)):\n",
    "    data_notrans_posthr.append(np.array(data_notrans[node]>(data_notrans_mean[node].max())))\n",
    "    data_notrans_negthr.append(np.array(data_notrans[node]<(data_notrans_mean[node].min())))\n",
    "    \n",
    "    data_notrans_thr.append(np.array(data_notrans_posthr[node] + data_notrans_negthr[node]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tavg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_notrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_notrans_posthr.append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_notrans_posthr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_notrans_mean[node].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_notrans_mean[node].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_notrans) #= tavg_time[range(9800,12000)]\n",
    "#len(data_notrans_mean)#= tavg_data.squeeze().T[:,range(9800, 10000)]\n",
    "#data_notrans#= tavg_data.squeeze().T[:,range(9800, 12000)]\n",
    "\n",
    "#data_notrans_posthr#= []\n",
    "#data_notrans_negthr= []\n",
    "#data_notrans_thr= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_notrans[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tavg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_notrans.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_notrans_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#time vs temporal average graph\n",
    "figure(figsize=(11,7))\n",
    "plot(tavg_time, datas,'k', alpha = 0.5)\n",
    "# plot(tavg_time, v1, 'r', alpha = 1)   #stimulation onset at 10000ms\n",
    "plot(tavg_time, follow, 'g', alpha =0.5)\n",
    "axes=plt.gca()\n",
    "# axes.set_xlim([9950,10700])\n",
    "axes.set_ylim([-0.23, 0.23])\n",
    "ylabel(\"Temporal average\")\n",
    "xlabel (\"Time(ms)\")\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(11,7))\n",
    "plot(tavg_time, datas,'k', alpha = 0.5)\n",
    "# plot(tavg_time, v1, 'red', alpha = 1)\n",
    "plot(tavg_time, follow, 'g', alpha =0.5)\n",
    "axes=plt.gca()\n",
    "axes.set_xlim([9800,12900]) #([9950,10700])\n",
    "axes.set_ylim([-0.01, 0.01])\n",
    "ylabel(\"Temporal average\")\n",
    "xlabel (\"Time(ms)\")\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding activation times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be finding the activation times using the positive and negative thresholds. We have already compared the data with the thresholds to see what times it will surpass them. We will first find them for all nodes followed by calculating them for the specified nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activation_time=np.array(np.zeros(len(data_notrans_thr)))\n",
    "for node in range(len(data_notrans_thr)):\n",
    "    for time in range(9800,15000):\n",
    "        if np.array(data_notrans_thr[node][time -9800]) == True:\n",
    "            activation_time[node]=time\n",
    "            break\n",
    "        else:\n",
    "            activation_time[node]= nan\n",
    "#activation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the times to see if they match the results from the empircal data, if not change the parameters and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#s = [113,122,123,124,145,146,147,149]\n",
    "s = [36,45,46,47,68,69,70,72] \n",
    "#s = [29,19,36,8,20,1,2,3,4,5,7,9,12,40,56,66,67,22,72,69,13,14,35,61,29,30,57, 50]\n",
    "activation_time_spec=np.array(np.zeros(len(s)))\n",
    "i= 0\n",
    "\n",
    "for node in s: \n",
    "    for time in range(9800,15000):\n",
    "        if np.array(data_notrans_thr[node][time -9800]) == True:\n",
    "            activation_time_spec[i]=time\n",
    "            break\n",
    "        else:\n",
    "            activation_time_spec[i]= nan\n",
    "    i += 1\n",
    "activation_time_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation_times_spec=np.zeros(len(activation_time_spec))\n",
    "for i in range(len(activation_time_spec)):\n",
    "    if activation_time_spec[i] == 0 or activation_time_spec[i] < 10000:\n",
    "        activation_times_spec[i]= nan\n",
    "    else:\n",
    "        activation_times_spec[i]= activation_time_spec[i] - 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd '/rri_disks/home/akacollja/stim_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_name= stim_name + '_data.txt'\n",
    "np.savetxt(file_name, activation_times_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in region labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in region labels and place them in a dataframe for easy manipulation later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',200)\n",
    "df_tavg = pd.DataFrame(np.squeeze(tavg_data),index=tavg_time)\n",
    "\n",
    "labs= pd.read_csv('/home/htian/Data/kelly_matrix/labelnames.txt',sep='\\t')['acronym'].values\n",
    "\n",
    "lh_labs = [l + '_L' for l in labs]\n",
    "rh_labs = [l + '_R' for l in labs]\n",
    "region_labels_lr = np.concatenate([lh_labs, rh_labs])\n",
    "\n",
    "df_tavg_labs = df_tavg.copy()\n",
    "df_tavg_labs.columns= region_labels_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tavg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be graphing a bunch of bar graphs for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation times of all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the activation times we will want to start displaying the time of the stimulation as 0. To do this we will need to loop through the activation times and subtract 10000 from each value. If the value is already less than 10000 we will replace it with nan since it does not activate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation_times=np.zeros(len(activation_time))\n",
    "\n",
    "for i in range(len(activation_time)):\n",
    "    if activation_time[i] == 0 or activation_time[i] < 10000:\n",
    "        activation_times[i]= nan\n",
    "    else:\n",
    "        activation_times[i]= activation_time[i] - 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data doesn't include the stimulated node, we will need to remove that label from our labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst_nodes = [i for i in range(154) if i != v1_node]\n",
    "labels_nostim=region_labels_lr[lst_nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just load the labels and activation times into a dataframe. Then, sort the activation times in ascending order. Finally, plot the ordered activation times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_activation= pd.DataFrame(activation_times, region_labels_lr) \n",
    "\n",
    "df_activation_sort = df_activation.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = df_activation_sort.plot(kind='barh', title=\"distances\", figsize=(17,17), legend=False)\n",
    "y_pos = np.arange(len(df_activation_sort))\n",
    "width= 0.35\n",
    "rects = ax.barh(y_pos, df_activation_sort, width)\n",
    "\n",
    "ax.set_ylabel('node acronyms')\n",
    "ax.set_xlabel('time activation(ms)')\n",
    "ax.invert_yaxis()\n",
    "ax.set_title('Node Activation Times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation times of specific nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the same process for your specified nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation_times_spec=np.zeros(len(activation_time_spec))\n",
    "\n",
    "for i in range(len(activation_time_spec)):\n",
    "    if activation_time_spec[i] == 0 or activation_time_spec[i] < 10000:\n",
    "        activation_times_spec[i]= 0\n",
    "    else:\n",
    "        activation_times_spec[i]= activation_time_spec[i] - 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you will need to manually include your label names in the same order as before or feel free to write a loop to include the labels that you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lab_spec= np.array(['FEF_L', 'MST(45)', 'MST(46)', 'MT_L', 'V1_L', 'V2_L', 'V3_L', 'V4_L'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_activation_spec= pd.DataFrame(activation_times_spec, lab_spec) \n",
    "\n",
    "df_activation_spec_sort = df_activation_spec.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_activation_spec_sort.plot(kind='barh', title=\"distances\", figsize=(6,3), legend=False)\n",
    "y_pos = np.arange(len(df_activation_spec_sort))\n",
    "width= 0.05\n",
    "rects = ax.barh(y_pos, df_activation_spec_sort, width)\n",
    "\n",
    "ax.set_ylabel('node acronyms')\n",
    "ax.set_xlabel('time activation(ms)')\n",
    "ax.invert_yaxis()\n",
    "ax.set_title('Node Activation Times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be making a bar graph to display the distances in order of closest to farthest nodes from our stimulated node (V1 for this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i =v1_node\n",
    "distances= np.array(np.zeros(len(new_conn.tract_lengths)))\n",
    "\n",
    "for j in range(len(new_conn.tract_lengths)):\n",
    "    distances[j]= new_conn.tract_lengths[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will need to slice the distances array at 77 because we only care about the distances of the nodes in the left hemisphere. Load them into a dataframe then sort. Then, plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_distances= pd.DataFrame([labs, distances[:77]]).T\n",
    "\n",
    "df_distances_labs= df_distances.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_distances_labs.plot(kind='barh', title=\"distances\", figsize=(10,10), legend=False)\n",
    "\n",
    "y_pos = np.arange(len(distances[:77]))\n",
    "width= 0.35\n",
    "rects = ax.barh(y_pos,df_distances_labs[1], width)\n",
    "ax.set_ylabel('node acronyms')\n",
    "ax.set_xlabel('Distances')\n",
    "ax.set_title('Distance from node \"V1\"')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax.set_yticks(range(len(labs)))\n",
    "ytickNames = ax.set_yticklabels(df_distances_labs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding amplitudes of each node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the amplitudes we will need to loop through the data (excluding the stimulated node) and make an array of each node's max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#no stim node in this data set\n",
    "max_amp= []\n",
    "for node in range(len(nostimnode.T)):\n",
    "    max_amp.append(max(abs(nostimnode.T[node, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_max_amp= pd.DataFrame(max_amp, labels_nostim) \n",
    "\n",
    "df_max_amp_sort= df_max_amp.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_max_amp_sort.plot(kind='barh', title=\"amplitudes\", figsize=(10,10), legend=False)\n",
    "y_pos = np.arange(len(df_max_amp_sort))\n",
    "width= 0.35\n",
    "rects = ax.barh(y_pos, df_max_amp_sort, width)\n",
    "\n",
    "ax.set_ylabel('node acronyms')\n",
    "ax.set_xlabel('amplitude')\n",
    "ax.invert_yaxis()\n",
    "ax.set_title('Amplitudes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another way to visualize the dissipation of the stimulus is to use a heatmap. It is important to adjust vmin and vmax accordingly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(df_tavg_labs.ix[9800:12000].T, ax=ax, vmin=-0.001, vmax=0.001, xticklabels='')\n",
    "ax.imshow(df_tavg.ix[9800:12000].T.values,vmin=-0.001, vmax=0.001, aspect='auto',cmap='coolwarm',\n",
    "          interpolation='none')\n",
    "ax.set_yticklabels\n",
    "ax.grid('off')\n",
    "for label in ax.get_yticklabels(): label.set_rotation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to calculate the covariance values 200ms before the stimulationand plot them on a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_notrans_prestim= np.squeeze(nostimnode.T)[:,range(9800, 10000)]\n",
    "dat_notrans_cov = np.cov(dat_notrans_prestim)\n",
    "\n",
    "fig, ax= plt.subplots(figsize= (9,9))\n",
    "fig.canvas.draw()\n",
    "ax.set_xticks(range(len(labels_nostim)))\n",
    "ax.set_yticks(range(len(labels_nostim)))\n",
    "ax.set_xticklabels(labels_nostim,rotation=90,fontsize=8)\n",
    "ax.set_yticklabels(labels_nostim, fontsize=8)\n",
    "plt.imshow(dat_notrans_cov, cmap= 'jet')\n",
    "plt.grid('off')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvector matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Eigen vector matrix from covariance matrix and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eig\n",
    "\n",
    "dat_notrans_cov_evals, dat_notrans_cov_evecs = np.linalg.eig(dat_notrans_cov)\n",
    "\n",
    "fig, ax =plt.subplots(figsize= (9,9))\n",
    "fig.canvas.draw()\n",
    "ax.set_xticks(range(len(labels_nostim)))\n",
    "ax.set_yticks(range(len(labels_nostim)))\n",
    "ax.set_xticklabels(labels_nostim,rotation=90,fontsize=8)\n",
    "ax.set_yticklabels(labels_nostim, fontsize= 8)\n",
    "\n",
    "dat_notrans_cov_evecs_154 = np.zeros([154,154])\n",
    "dat_notrans_cov_evecs_154[:153,:153] = dat_notrans_cov_evecs\n",
    "plt.imshow(dat_notrans_cov_evecs_154, cmap= 'jet')\n",
    "plt.grid('off')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, our data will require a null entry so we will create a version of our eigenvector matrix that has 155 nodes as the last one will be for the null entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_notrans_cov_evecs_155 = np.zeros([155,155])\n",
    "dat_notrans_cov_evecs_155[:153,:153] = dat_notrans_cov_evecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Slice on Brain Surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow the dissipation visually, we will be plotting the data on a brain surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tvb.datatypes.cortex import Cortex\n",
    "from tvb.datatypes.region_mapping import RegionMapping\n",
    "from tvb.datatypes.projections import ProjectionMatrix\n",
    "import pandas as pd\n",
    "from matplotlib.tri import Triangulation\n",
    "from numpy import pi, cos, sin\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_surface_mpl(vtx,tri,data=None,rm=None,reorient=None,view='superior',\n",
    "                     shaded=False,ax=None,figsize=(6,4), title=None,\n",
    "                     lthr=None,uthr=None, nz_thr = 1E-20,\n",
    "                     shade_kwargs = {'edgecolors': 'k', 'linewidth': 0.1,\n",
    "                                     'alpha': None, 'cmap': 'coolwarm',\n",
    "                                     'vmin': None, 'vmax': None}):\n",
    "\n",
    "    # in the namespace inadvertently. \n",
    "    vtx,tri = vtx.copy(),tri.copy()\n",
    "    if data is not None: data = data.copy()\n",
    "\n",
    "    # 1. Set the viewing angle \n",
    "  \n",
    "    if reorient == 'tvb':\n",
    "        # The tvb default brain has coordinates in the order \n",
    "        # yxz for some reason. So first change that:   \n",
    "        vtx = np.array([vtx[:,1],vtx[:,0],vtx[:,2]]).T.copy()\n",
    "        # Also need to reflect in the x axis\n",
    "        vtx[:,0]*=-1\n",
    "\n",
    "    # (reorient == 'fs' is same as reorient=None; so not strictly needed\n",
    "    #  but is included for clarity)\n",
    "\n",
    "    # ...get rotations for standard view options\n",
    "    \n",
    "    if   view == 'lh_lat'    : rots =  [(0,-90),(1,90)  ]\n",
    "    elif view == 'lh_med'    : rots =  [(0,-90),(1,-90) ] \n",
    "    elif view == 'rh_lat'    : rots =  [(0,-90),(1,-90) ]\n",
    "    elif view == 'rh_med'    : rots =  [(0,-90),(1,90)  ]\n",
    "    elif view == 'superior'  : rots =   None\n",
    "    elif view == 'inferior'  : rots =   (1,180)\n",
    "    elif view == 'anterior'  : rots =   (0,-90)\n",
    "    elif view == 'posterior' : rots =  [(0, -90),(1,180)]\n",
    "    elif (type(view) == tuple) or (type(view) == list): rots = view \n",
    "\n",
    "    # (rh_lat is the default 'view' argument because no rotations are \n",
    "    #  for that one; so if no view is specified when the function is called, \n",
    "    #  the 'rh_lat' option is chose here and the surface is shown 'as is'                          \n",
    "                            \n",
    "    # ...apply rotations                          \n",
    "    if rots is None: rotmat = np.eye(3)\n",
    "    else:            rotmat = get_combined_rotation_matrix(rots)\n",
    "    vtx = np.dot(vtx,rotmat)\n",
    "\n",
    "    # 2. Sort out the data\n",
    "                                    \n",
    "    # ...if no data is given, plot a vector of 1s. \n",
    "    #    if using region data, create corresponding surface vector \n",
    "    if data is None: \n",
    "        data = np.ones(vtx.shape[0]) \n",
    "    elif data.shape[0] != vtx.shape[0]: \n",
    "        data = np.array([data[r] for r in rm])\n",
    "    \n",
    "    # ...apply thresholds\n",
    "    if uthr: data *= (data < uthr)\n",
    "    if lthr: data *= (data > lthr)\n",
    "    data *= (np.abs(data) > nz_thr)\n",
    "\n",
    "    # 3. Create the surface triangulation object \n",
    "    x,y,z = vtx.T\n",
    "    tx,ty,tz = vtx[tri].mean(axis=1).T\n",
    "    tr = Triangulation(x,y,tri[np.argsort(tz)])\n",
    "                \n",
    "    # 4. Make the figure \n",
    "    if ax is None: fig, ax = plt.subplots(figsize=figsize)  \n",
    "  \n",
    "    #if shade = 'gouraud': shade_opts['shade'] = \n",
    "    tc = ax.tripcolor(tr, np.squeeze(data), **shade_kwargs)\n",
    "                        \n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    if title is not None: ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_surface_mpl_mv(vtx=None,tri=None,data=None,rm=None,hemi=None,   # Option 1\n",
    "                        vtx_lh=None,tri_lh=None,data_lh=None,rm_lh=None, # Option 2\n",
    "                        vtx_rh=None,tri_rh=None,data_rh=None,rm_rh=None,\n",
    "                        title=None,**kwargs):\n",
    " \n",
    "    if vtx is not None:                                    # Option 1\n",
    "        tri_hemi = hemi[tri].any(axis=1)\n",
    "        tri_lh,tri_rh = tri[tri_hemi==0],tri[tri_hemi==1]\n",
    "    elif vtx_lh is not None:                               # Option 2\n",
    "        vtx = np.vstack([vtx_lh,vtx_rh])\n",
    "        tri = np.vstack([tri_lh,tri_rh+tri_lh.max()+1])\n",
    "\n",
    "    if data_lh is not None:                                # Option 2\n",
    "        data = np.hstack([data_lh,data_rh])\n",
    "    \n",
    "    if rm_lh is not None:                                  # Option 2 \n",
    "        rm = np.hstack([rm_lh,rm_rh + rm_lh.max() + 1])\n",
    "    \n",
    " \n",
    "    # 2. Now do the plots for each view\n",
    "\n",
    "    # (Note: for the single hemispheres we only need lh/rh arrays for the \n",
    "    #  faces (tri); the full vertices, region mapping, and data arrays\n",
    "    #  can be given as arguments, they just won't be shown if they aren't \n",
    "    #  connected by the faces in tri )\n",
    "  \n",
    "    # LH lateral\n",
    "    plot_surface_mpl(vtx,tri_lh,data=data,rm=rm,view='lh_lat',\n",
    "                   ax=subplot(2,3,1),**kwargs)\n",
    "    \n",
    "    # LH medial\n",
    "    plot_surface_mpl(vtx,tri_lh, data=data,rm=rm,view='lh_med',\n",
    "                   ax=subplot(2,3,4),**kwargs)\n",
    "    \n",
    "    # RH lateral\n",
    "    plot_surface_mpl(vtx,tri_rh, data=data,rm=rm,view='rh_lat',\n",
    "                   ax=subplot(2,3,3),**kwargs)\n",
    "    \n",
    "    # RH medial\n",
    "    plot_surface_mpl(vtx,tri_rh, data=data,rm=rm,view='rh_med',\n",
    "                   ax=subplot(2,3,6),**kwargs)\n",
    "    \n",
    "    # Both superior\n",
    "    plot_surface_mpl(vtx,tri, data=data,rm=rm,view='superior',\n",
    "                   ax=subplot(1,3,2),title=title,**kwargs)\n",
    "    \n",
    "    plt.subplots_adjust(left=0.0, right=1.0, bottom=0.0,\n",
    "                      top=1.0, wspace=0, hspace=0) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_combined_rotation_matrix(rotations):\n",
    "    rotmat = np.eye(3)\n",
    "    \n",
    "    if type(rotations) is tuple: rotations = [rotations] \n",
    "    for r in rotations:\n",
    "        newrot = get_rotation_matrix(r[0],r[1])\n",
    "        rotmat = np.dot(rotmat,newrot)\n",
    "    return rotmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rotation_matrix(rotation_axis, deg):\n",
    "    # (note make deg minus to change from anticlockwise to clockwise rotation)\n",
    "    th = -deg * (pi/180) # convert degrees to radians\n",
    "    \n",
    "    if rotation_axis == 0:\n",
    "        return np.array( [[    1,         0,         0    ],\n",
    "                          [    0,      cos(th),   -sin(th)],\n",
    "                          [    0,      sin(th),    cos(th)]])\n",
    "    elif rotation_axis ==1:\n",
    "        return np.array( [[   cos(th),    0,        sin(th)],\n",
    "                          [    0,         1,          0    ],\n",
    "                          [  -sin(th),    0,        cos(th)]])\n",
    "    elif rotation_axis ==2:\n",
    "        return np.array([[   cos(th),  -sin(th),     0    ],\n",
    "                         [    sin(th),   cos(th),     0   ],\n",
    "                         [     0,         0,          1   ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willl now need to load in the region mapping files to help us create the plotting surface. Here, we have appended the path where our files are located. Our region mapping file works when manually loaded in so that is what we will do for our file but feel free to attach the zip file name instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx_file ='/home/htian/Data/newzip.zip'\n",
    "rm= np.loadtxt('/home/htian/Data/region_mapping.txt').astype(int)\n",
    "hemi= np.loadtxt('/home/htian/Data/fv91_srfData_20170215/hemispheres.txt').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = cortex.Cortex.from_file(source_file = ctx_file)\n",
    "vtx,tri = ctx.vertices,ctx.triangles\n",
    "df_tavg = pd.DataFrame(np.squeeze(tavg_data),index=tavg_time)\n",
    "df_tavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to normalize all values at their respective time. We will set the stimulated node to 0 because we only care about the amplitudes of all other nodes and we don't want it to be taken as the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notstim= np.squeeze(tavg_data).copy()\n",
    "notstim[:, 68]= 0\n",
    "m = np.array([(np.abs(notstim[i])).max() for i in range(len(notstim))])\n",
    "for i in range(len(notstim)):\n",
    "    for j in range(len(notstim[1])):\n",
    "        print i,\n",
    "        notstim[i,j] /= m[i]\n",
    "        \n",
    "df_notstim = pd.DataFrame(notstim,index=tavg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notstim.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notstim.values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notstim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notstim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tavg_time1=tavg_time[range(8800, 14000)]\n",
    "#tavg_data1=tavg_data[range(8800, 14000)]\n",
    "notstim= np.squeeze(tavg_data).copy()\n",
    "notstim[:, 68]= 0\n",
    "m = np.array([(np.abs(notstim[i])).max() for i in range(len(notstim))])\n",
    "for i in range(len(notstim)):\n",
    "    for j in range(len(notstim[1])):\n",
    "        print i,\n",
    "        notstim[i,j] /= m[i]\n",
    "        \n",
    "df_notstim = pd.DataFrame(notstim,index=tavg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CUT OUT THE TRANSIENT FROM THE RANGE\n",
    "# notstim_notrans=tavg_data.squeeze().T[:, range(5000,12000)]\n",
    "notstim_notrans=tavg_data.squeeze().T[:, range(5000,14000)] #cut out the transient/second oscillation\n",
    "# notstim_notrans[stim_node]=0.0\n",
    "# tavg_time_thr=tavg_time[range(5000, 12000)]\n",
    "tavg_time1=tavg_time[range(5000, 14000)]\n",
    "nodes=np.arange(154)\n",
    "\n",
    "m = np.array([(np.abs(notstim_notrans[i])).max() for i in range(len(notstim_notrans))])\n",
    "for i in range(len(notstim_notrans)):\n",
    "    for j in range(len(notstim_notrans[1])):\n",
    "        notstim_notrans[i,j] /= m[i] #for each time point(i) for each node(j), what is the proportionate activation of \n",
    "                                #that particular node compared its maximum activation\n",
    "#notstim_notrans[stim_node]=0\n",
    "\n",
    "notstim_notrans[:, stim_node]= 0\n",
    "\n",
    "df_notstim1 = pd.DataFrame(notstim_notrans.T, index=tavg_time1) #, columns=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notstim_notrans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notstim_notrans.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notstim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for prestimulation time block\n",
    "nostim=notstim_notrans[:,:9000]\n",
    "\n",
    "tavg_time_nostim=tavg_time[range(5000, 14000)]\n",
    "\n",
    "nostim_thr=pd.DataFrame(nostim.T, index=tavg_time_nostim, columns=nodes)\n",
    "nostim_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostim_thr.abs().values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin_node=nostim_thr.abs().values.mean()\n",
    "vmin_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostim_thr.abs().values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=5, nrows=10)\n",
    "cmap = cm.Reds\n",
    "cmap.set_under(color='w')\n",
    "vmin_node=nostim_thr.abs().values.mean()\n",
    "kws = {'edgecolors': 'k', 'vmin': vmin_node, 'cmap': cmap, #between 0.013 and 0.015 (0.014-0.015) #0.0145\n",
    "'vmax': 1, 'alpha': None, 'linewidth': 0.01}\n",
    "\n",
    "ts=[10000.5, 10010.5, 10070.5]\n",
    "\n",
    "for t_it,t in enumerate(ts):\n",
    "    data = np.append(df_notstim1.ix[t].abs().values, nan)\n",
    "#     for node in range(len(nodes)): ##only gives one value for vmin, and therefore plotting function\n",
    "#         vmin_node=vmin_thr[node]\n",
    "    \n",
    "    plot_surface_mpl_mv(vtx=vtx ,tri=tri,rm=rm,data=data, figsize=(10,10),\n",
    "                    hemi=hemi ,shade_kwargs=kws) \n",
    "    plt.title(stim_name + '_t=%1.1fms' %t,fontsize=9)\n",
    "    \n",
    "    f = '/rri_disks/home/akacollja/tvb/brain_plots/braintmp_ER_t%1.1fms.png' %t\n",
    "    plt.savefig(f,bbox_inches='tight', pad_inches=0, dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Make the plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be saving the figures then displaying them since these are a little big and have difficulties loading them on the spot. Expect at least a few minutes for the figures to load. Change the ts times to display the time frames you want to look at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd '/rri_disks/home/akacollja/tvb/brain_plots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=5, nrows=10)\n",
    "cmap = cm.Reds\n",
    "cmap.set_under(color='W')\n",
    "\n",
    "kws = {'edgecolors': 'k', 'vmin': -5, 'cmap': cmap, \n",
    "       'vmax': 18, 'alpha':None, 'linewidth': 0.01}\n",
    "\n",
    "#ts=[10000.5, 10050.5, 10100.5, 10150.5, 10200.5, 10250.5, 10300.5, 10350.5, 10400.5, 10450.5]\n",
    "\n",
    "#ts=[ 10000.5, 10045.5, 10350.5]\n",
    "\n",
    "ts=[10000.5]\n",
    "\n",
    "for t_it,t in enumerate(ts):\n",
    "\n",
    "    dat = np.append(df_notstim.ix[t].abs().values, nan)\n",
    "    #dat = np.append(df_tavg.ix[t].abs().values, nan)\n",
    "\n",
    "    \n",
    "    plot_surface_mpl_mv(vtx=vtx,tri=tri,rm=rm,data=dat, figsize=(10,10),\n",
    "                    hemi=hemi ,shade_kwargs=kws) \n",
    "    \n",
    "    plt.title(stim_name + '_t=%1.1fms' %t,fontsize=9)\n",
    "    \n",
    "    f = '/rri_disks/home/akacollja/tvb/brain_plots/braintmp_v1_t%1.1fms.png' %t\n",
    "    plt.savefig(f,bbox_inches='tight', pad_inches=0, dpi=1200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=10, figsize= (12,40))\n",
    "\n",
    "for t_it,t in enumerate(ts):\n",
    "    f =  '/rri_disks/home/akacollja/tvb/brain_plots/braintmp_v1_t%1.1fms.png' %t\n",
    "    ax[t_it].imshow(plt.imread(f))\n",
    "    ax[t_it].axis('off')\n",
    "    ax[t_it].set_title('t=%1.1fms' %t,fontsize=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be displaying figures at given eigen vector slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the data with one color\n",
    "\n",
    "kws = {'edgecolors': 'k', 'vmin': 0, 'cmap': 'Reds', #'vmin': 0.4\n",
    "       'vmax': 1, 'alpha': None, 'linewidth': 0.01}\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4,nrows=2,figsize=(12,6))\n",
    "\n",
    "for e_it in range(4):\n",
    "    dat = np.abs(dat_notrans_cov_evecs_155[:,e_it])\n",
    "\n",
    "    plot_surface_mpl(vtx=vtx,tri=tri,data=dat,rm=rm,ax=ax[0][e_it],\n",
    "                     shade_kwargs=kws,view='rh_lat', title='evec %s' %e_it)\n",
    "\n",
    "    plot_surface_mpl(vtx=vtx,tri=tri,data=dat,rm=rm,ax=ax[1][e_it],\n",
    "                     shade_kwargs=kws,view='superior')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HC stimulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the process with the node you want to stimulate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/akacollja/tvb_scientific/tvb-library')\n",
    "sys.path.append('/home/akacollja/tvb_scientific/tvb-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "\n",
    "import sys\n",
    "nbso,nbse = sys.stdout,sys.stderr # hack part 1/2 to keep output printing properly\n",
    "\n",
    "from tvb.simulator.lab import *\n",
    "#LOG= get_logger('demo')\n",
    "from tvb.simulator.plot.tools import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.stdout,sys.stderr = nbso,nbse  # ...hack part 2/2\n",
    "#from tvb.simulator.plot.tools import plot_surface_mpl,plot_surface_mpl_mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, glob,h5py, itertools, multiprocessing\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from scipy import optimize\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image,display as d\n",
    "import seaborn as sns\n",
    "\n",
    "import nibabel as nib\n",
    "from nilearn.plotting import plot_roi\n",
    "from dipy.tracking.utils import apply_affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab nbagg\n",
    "from tvb.simulator.lab import *\n",
    "LOG= get_logger('demo')\n",
    "from tvb.simulator.plot.tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters are the same as the ones above. If you change the parameters in the V1 stimulation, remember to change them here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_conn= connectivity.Connectivity.from_file(\"newconn.zip\")\n",
    "new_conn.configure()\n",
    "new_conn.weights= (np.loadtxt('/home/akacollja/Data/avg9/weights.txt')) #pre-lesion matrix\n",
    "#new_conn.weights= (np.loadtxt('/home/akacollja/Data/avg9/weightsLaplaceEquation.txt')) #weights file contains Laplace's equation\n",
    "#new_conn.weights= (np.loadtxt('/home/akacollja/LesionStimulation/newWeightsV4.txt')) #lesioned matrix\n",
    "new_conn.tract_lengths= np.loadtxt('/home/akacollja/Data/avg9/tract_lengths.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod= models.Generic2dOscillator(d = (1/1000.) * 76.74,\n",
    "                                tau =1., f= 1.,e= 0., g= -0.1, \n",
    "                            alpha =1., gamma= 1., c= 0., b= -12.3083, beta =0., a =0.)\n",
    "heunint = integrators.HeunDeterministic(dt = 0.1) \n",
    "#integrators = integrators.HeunDeterministic(dt = 0.2)\n",
    "#integrators = integrators.HeunStochastic(dt = 0.1, noise=noise.Additive(nsig=5e-15)) \n",
    "#integrators = integrators.HeunStochastic(dt = 0.1, noise=noise.Additive(nsig=0)) \n",
    "cpl = coupling.Linear(a=0.01198)\n",
    "mons= monitors.TemporalAverage(period = 1.0)\n",
    "new_conn.speed= 8.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the weighting of the stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name and node number of the node you want to stimulate ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stim_node= 30\n",
    "stim_name= 'CA3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to change these values if you change them above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weighting = numpy.zeros((154))\n",
    "weighting [[stim_node]] = 0.03\n",
    "\n",
    "#temporal profile\n",
    "eqn_t = equations.PulseTrain()\n",
    "eqn_t.parameters['onset'] = 10000\n",
    "eqn_t.parameters['T'] = 5000.0\n",
    "eqn_t.parameters['tau'] = 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stimulus = patterns.StimuliRegion( temporal = eqn_t, \n",
    "                                  connectivity = new_conn, \n",
    "                                  weight = weighting)\n",
    "\n",
    "stimulus.configure_space()\n",
    "stimulus.configure_time(numpy.arange(0.,15000, 2**-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting together the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = simulator.Simulator(\n",
    "        model = mod,\n",
    "        connectivity = new_conn,\n",
    "        coupling = cpl,\n",
    "        integrator = heunint,\n",
    "        monitors = mons,\n",
    "        stimulus = stimulus, \n",
    "        simulation_length = 15000,\n",
    "        ).configure()\n",
    "\n",
    "\n",
    "\n",
    "#sim.initial_conditions = init\n",
    "\n",
    "sim.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = 0.1*np.ones(sim.good_history_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = simulator.Simulator(\n",
    "        model = mod,\n",
    "        connectivity = new_conn,\n",
    "        coupling = cpl,\n",
    "        integrator = heunint,\n",
    "        monitors = mons,\n",
    "        stimulus = stimulus, \n",
    "        simulation_length = 15000,\n",
    "        ).configure()\n",
    "\n",
    "\n",
    "\n",
    "sim.initial_conditions = init\n",
    "\n",
    "sim.configure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(tavg_time, tavg_data), = sim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst_nodes = [i for i in range(154) if i != stim_node]\n",
    "nostimnode=tavg_data[:, 0, lst_nodes, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas_raw= (nostimnode)\n",
    "d_raw= (tavg_data[:, 0, stim_node, 0])\n",
    "follow_raw=( tavg_data[:, 0, 19, 0],tavg_data[:, 0, 36, 0],tavg_data[:, 0, 8, 0],tavg_data[:, 0, 20, 0],\n",
    "             tavg_data[:, 0, 1, 0],tavg_data[:, 0, 2, 0],tavg_data[:, 0, 3, 0],tavg_data[:, 0, 4, 0],\n",
    "             tavg_data[:, 0, 5, 0], tavg_data[:, 0, 7, 0], tavg_data[:, 0, 9, 0],tavg_data[:, 0, 12, 0],\n",
    "             tavg_data[:, 0, 40, 0],tavg_data[:, 0, 56, 0],tavg_data[:, 0, 66, 0],tavg_data[:, 0, 67, 0],\n",
    "             tavg_data[:, 0, 22, 0],tavg_data[:, 0, 72, 0],tavg_data[:, 0, 69, 0],tavg_data[:, 0, 13, 0],\n",
    "             tavg_data[:, 0, 14, 0],tavg_data[:, 0, 35, 0],tavg_data[:, 0, 61, 0],tavg_data[:, 0, 29, 0],\n",
    "             tavg_data[:, 0, 30, 0], tavg_data[:, 0, 57, 0])\n",
    "datas= np.squeeze(datas_raw)\n",
    "d= np.squeeze(d_raw).T\n",
    "follow = np.squeeze(follow_raw).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas_raw= (nostimnode)\n",
    "d_raw= (tavg_data[:, 0, stim_node, 0])\n",
    "follow_raw=( tavg_data[:, 0, 30, 0],#tavg_data[:, 0, 29, 0],tavg_data[:, 0, 35, 0],tavg_data[:, 0, 61, 0],\n",
    "             tavg_data[:, 0, 66, 0],)\n",
    "datas= np.squeeze(datas_raw)\n",
    "d= np.squeeze(d_raw).T\n",
    "follow = np.squeeze(follow_raw).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#time vs temporal average graph\n",
    "figure()\n",
    "plot(tavg_time, datas,'k', alpha = 0.5)\n",
    "#plot(tavg_time, v1, 'r', alpha = 1)\n",
    "plot(tavg_time, follow, 'g', alpha =0.5)\n",
    "axes=plt.gca()\n",
    "#axes.set_ylim([-0.15, 0.15])\n",
    "ylabel(\"Temporal average\")\n",
    "xlabel (\"Time(ms)\")\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#time vs temporal average graph\n",
    "figure()\n",
    "plot(tavg_time, datas,'k', alpha = 0.5)\n",
    "#plot(tavg_time, d, 'r', alpha = 1)\n",
    "plot(tavg_time, follow, 'g', alpha =0.5)\n",
    "ylabel(\"Temporal average\")\n",
    "xlabel (\"Time(ms)\")\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=(11,7))\n",
    "plot(tavg_time, datas,'k', alpha = 0.5)\n",
    "#plot(tavg_time, d, 'red', alpha = 1)\n",
    "plot(tavg_time, follow, 'g', alpha =0.5)\n",
    "axes=plt.gca()\n",
    "axes.set_xlim([9950,10500]) #([9950,10700])\n",
    "axes.set_ylim([-0.02, 0.02])\n",
    "ylabel(\"Temporal average\")\n",
    "xlabel (\"Time(ms)\")\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#time vs temporal average graph\n",
    "figure()\n",
    "plot(tavg_time, datas,'k', alpha = 0.5)\n",
    "#plot(tavg_time, d, 'red', alpha = 1)\n",
    "plot(tavg_time, follow, 'g', alpha =0.5)\n",
    "axes=plt.gca()\n",
    "axes.set_xlim([9950,15000]) #([9950,10700])\n",
    "axes.set_ylim([-0.02, 0.02])\n",
    "ylabel(\"Temporal average\")\n",
    "xlabel (\"Time(ms)\")\n",
    "plt.grid('off')\n",
    "\n",
    "#savefig('/home/akacollja/tvb/fig_dir/timeseries_V1scaled', bbox_inches='tight')\n",
    "\n",
    "#savefig('/hestia/ryan_lab/byang/Byang/deterministic/timeseries_V1scaled.png', bbox_inches='tight')\n",
    "#savefig('/home/byang/fig_dir/timeseries_V1scaled', bbox_inches='tight')\n",
    "\n",
    "\n",
    "# #axes.get_xlim[10000,11000]\n",
    "# \n",
    "# get_xlim(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_notrans = tavg_time[range(9800,15000)]\n",
    "data_notrans_mean= tavg_data.squeeze().T[:,range(9800, 10000)]\n",
    "data_notrans= tavg_data.squeeze().T[:,range(9800, 15000)]\n",
    "\n",
    "data_notrans_posthr= []\n",
    "data_notrans_negthr= []\n",
    "data_notrans_thr= []\n",
    "\n",
    "for node in range(len(data_notrans_mean)):\n",
    "    data_notrans_posthr.append(np.array(data_notrans[node]>(data_notrans_mean[node].max())))\n",
    "    data_notrans_negthr.append(np.array(data_notrans[node]<(data_notrans_mean[node].min())))\n",
    "    data_notrans_thr.append(np.array(data_notrans_posthr[node] + data_notrans_negthr[node]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name= stim_name + '_data.txt'\n",
    "np.savetxt(file_name, data_notrans_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_notrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding activation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation_time=np.array(np.zeros(len(data_notrans_thr)))\n",
    "for node in range(len(data_notrans_thr)):\n",
    "    for time in range(9800,15000):\n",
    "        if np.array(data_notrans_thr[node][time -9800]) == True:\n",
    "            activation_time[node]=time\n",
    "            break\n",
    "        else:\n",
    "            activation_time[node]= nan\n",
    "#activation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = [stim_node, 30,29,61,57,50,35,13,14,66,67,22,72,69,40,56,20,1,2,3,4,5,7,9,12,8,19,36]\n",
    "#s = [stim_node, ]\n",
    "#s = [stim_node,19,36,8,20,1,2,3,4,5,7,9,12,40,56,66,67,22,72,69,13,14,35,61,29,30,57, 50]\n",
    "#nodes used by Heather s = [stim_node,36,69,72,22,13,14,40,66,8,19, 18,3,56,49,1,2,61,30,29,35]\n",
    "#s = [stim_node,96,113,85,97,78,79,80,81,82,84,86,89,117,133,143,144,99,149,146,90,91,112,138,106,107]\n",
    "activation_time_spec=np.array(np.zeros(len(s)))\n",
    "i= 0\n",
    "for node in s: \n",
    "    for time in range(9800,15000):\n",
    "        if np.array(data_notrans_thr[node][time -9800]) == True:\n",
    "            activation_time_spec[i]=time\n",
    "            break\n",
    "        else:\n",
    "            activation_time_spec[i]= nan\n",
    "    i += 1\n",
    "activation_time_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation_times_spec=np.zeros(len(activation_time_spec))\n",
    "for i in range(len(activation_time_spec)):\n",
    "    if activation_time_spec[i] == 0 or activation_time_spec[i] < 10000:\n",
    "        activation_times_spec[i]= nan\n",
    "    else:\n",
    "        activation_times_spec[i]= activation_time_spec[i] - 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the activation times externally for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cd '/rri_disks/home/akacollja/LesionStimulation/stim_data/area23Lesion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd '/rri_disks/home/akacollja/stim_data/newStim_July2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name= stim_name + '_data.txt'\n",
    "np.savetxt(file_name, activation_times_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in region labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',200)\n",
    "df_tavg = pd.DataFrame(np.squeeze(tavg_data),index=tavg_time)\n",
    "labs= pd.read_csv('/home/htian/Data/kelly_matrix/labelnames.txt',sep='\\t')['acronym'].values\n",
    "lh_labs = [l + '_L' for l in labs]\n",
    "rh_labs = [l + '_R' for l in labs]\n",
    "region_labels_lr = np.concatenate([lh_labs, rh_labs])\n",
    "df_tavg_labs = df_tavg.copy()\n",
    "df_tavg_labs.columns= region_labels_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation_times=np.zeros(len(activation_time))\n",
    "\n",
    "for i in range(len(activation_time)):\n",
    "    if activation_time[i] == 0 or activation_time[i] < 10000:\n",
    "        activation_times[i]= nan\n",
    "    else:\n",
    "        activation_times[i]= activation_time[i] - 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lab_spec= np.array(['stim' ,'46', 'FEF', '24', '5', '10', '11', '12', '13', '14', '23', \n",
    "                    '25', '32', 'Ig', 'Pro', 'TF', 'TH', '7a', 'V4', 'V2', '35', '36', \n",
    "                    'ER', 'S', 'CA1', 'CA3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_activation_spec= pd.DataFrame(activation_times, region_labels_lr) \n",
    "lst_nodes = [i for i in range(154) if i != stim_node]\n",
    "labels_nostim=region_labels_lr[lst_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_activation_spec_sort = df_activation_spec.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_activation_spec_sort = df_activation_spec.sort_values(0)\n",
    "ax = df_activation_spec_sort.plot(kind='barh', title=\"distances\", figsize=(9,6), legend=False)\n",
    "y_pos = np.arange(len(df_activation_spec_sort))\n",
    "width= 0.05\n",
    "rects = ax.barh(y_pos, df_activation_spec_sort, width)\n",
    "\n",
    "ax.set_ylabel('node acronyms')\n",
    "ax.set_xlabel('time activation(ms)')\n",
    "ax.invert_yaxis()\n",
    "ax.set_title('Node Activation Times')\n",
    "ax.set_xlim(0,1700)\n",
    "\n",
    "\n",
    "savefig ('/home/akacollja/activation', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distances from stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i =stim_node \n",
    "distances= np.array(np.zeros(len(new_conn.tract_lengths)))\n",
    "\n",
    "for j in range(len(new_conn.tract_lengths)): \n",
    "    distances[j]= new_conn.tract_lengths[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_distances= pd.DataFrame([labs, distances[:77]]).T\n",
    "df_distances_labs= df_distances.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_distances_labs.plot(kind='barh', title=\"distances\", figsize=(10,10), legend=False)\n",
    "y_pos = np.arange(len(distances[:77])) \n",
    "width= 0. \n",
    "rects = ax.barh(y_pos,df_distances_labs[1], width) \n",
    "ax.set_ylabel('node acronyms') \n",
    "ax.set_xlabel('Distances')\n",
    "ax.set_title('Distance from stim node') \n",
    "ax.invert_yaxis()\n",
    "ax.set_yticks(range(len(labs))) \n",
    "ytickNames = ax.set_yticklabels(df_distances_labs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding amplitudes of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#no stim node in this data set \n",
    "data_nostim= nostimnode.T\n",
    "\n",
    "max_amp= [] \n",
    "for node in range(len(data_nostim)): \n",
    "    max_amp.append(max(abs(data_nostim[node, range(9800,10000)])))\n",
    "\n",
    "df_max_amp= pd.DataFrame(max_amp, labels_nostim) \n",
    "df_max_amp_sort= df_max_amp.sort_values(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_max_amp_sort.plot(kind='barh', title=\"amplitudes\", figsize=(10,10), legend=False) \n",
    "y_pos = np.arange(len(df_max_amp_sort)) \n",
    "width= 0. \n",
    "rects = ax.barh(y_pos, df_max_amp_sort, width)\n",
    "ax.set_ylabel('node acronyms') \n",
    "ax.set_xlabel('amplitude') \n",
    "ax.invert_yaxis() \n",
    "ax.set_title('Amplitudes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "fig, ax = plt.subplots(figsize = (10, 10)) \n",
    "sns.heatmap(df_tavg_labs.ix[9800:12000].T, ax=ax, vmin=-0.001, vmax=0.001, xticklabels='') \n",
    "ax.imshow(df_tavg.ix[9800:12000].T.values,vmin=-0.001, vmax=0.001, \n",
    "          aspect='auto',cmap='coolwarm', interpolation='none') \n",
    "ax.set_yticklabels \n",
    "ax.grid('off') \n",
    "for label in ax.get_yticklabels(): \n",
    "    label.set_rotation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_raw= (nostimnode).T \n",
    "dat_notrans_prestim= np.squeeze(dat_raw)[:,range(9800, 10000)] \n",
    "dat_notrans_prestim.shape\n",
    "\n",
    "lst_nodes = [i for i in range(154) if i != stim_node] \n",
    "labels_nostim=region_labels_lr[lst_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(figsize= (9,9)) \n",
    "fig.canvas.draw() \n",
    "ax.set_xticks(range(len(labels_nostim))) \n",
    "ax.set_yticks(range(len(labels_nostim))) \n",
    "ax.set_xticklabels(labels_nostim,rotation=90,fontsize=8) \n",
    "ax.set_yticklabels(labels_nostim, fontsize=8) \n",
    "plt.imshow(np.cov(dat_notrans_prestim), cmap= 'jet') \n",
    "plt.grid('off') \n",
    "plt.colorbar()\n",
    "\n",
    "#savefig('/rri_disks/home/akacollja/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen vector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eig\n",
    "\n",
    "dat_notrans_cov = np.cov(dat_notrans_prestim) \n",
    "dat_notrans_cov_evals, dat_notrans_cov_evecs = np.linalg.eig(dat_notrans_cov)\n",
    "fig, ax =plt.subplots(figsize= (9,9)) \n",
    "fig.canvas.draw() \n",
    "ax.set_xticks(range(len(labels_nostim))) \n",
    "ax.set_yticks(range(len(labels_nostim))) \n",
    "ax.set_xticklabels(labels_nostim,rotation=90,fontsize=8) \n",
    "ax.set_yticklabels(labels_nostim, fontsize= 8)\n",
    "\n",
    "#changed all 154 to 155 to take 0 into account \n",
    "dat_notrans_cov_evecs_155 = np.zeros([155,155]) \n",
    "dat_notrans_cov_evecs_155[:153,:153] = dat_notrans_cov_evecs \n",
    "plt.imshow(dat_notrans_cov_evecs_155, cmap= 'jet') \n",
    "plt.grid('off') \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time slice on brain using surface plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tvb.datatypes.cortex import Cortex\n",
    "from tvb.datatypes.region_mapping import RegionMapping\n",
    "from tvb.datatypes.projections import ProjectionMatrix\n",
    "import pandas as pd\n",
    "from matplotlib.tri import Triangulation\n",
    "from numpy import pi, cos, sin\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_surface_mpl(vtx,tri,data=None,rm=None,reorient=None,view='superior',\n",
    "                     shaded=False,ax=None,figsize=(6,4), title=None,\n",
    "                     lthr=None,uthr=None, nz_thr = 1E-20,\n",
    "                     shade_kwargs = {'edgecolors': 'k', 'linewidth': 0.1,\n",
    "                                     'alpha': None, 'cmap': 'coolwarm',\n",
    "                                     'vmin': None, 'vmax': None}):\n",
    "\n",
    "    # in the namespace inadvertently. \n",
    "    vtx,tri = vtx.copy(),tri.copy()\n",
    "    if data is not None: data = data.copy()\n",
    "\n",
    "    # 1. Set the viewing angle \n",
    "  \n",
    "    if reorient == 'tvb':\n",
    "        # The tvb default brain has coordinates in the order \n",
    "        # yxz for some reason. So first change that:   \n",
    "        vtx = np.array([vtx[:,1],vtx[:,0],vtx[:,2]]).T.copy()\n",
    "        # Also need to reflect in the x axis\n",
    "        vtx[:,0]*=-1\n",
    "\n",
    "    # (reorient == 'fs' is same as reorient=None; so not strictly needed\n",
    "    #  but is included for clarity)\n",
    "\n",
    "    # ...get rotations for standard view options\n",
    "    \n",
    "    if   view == 'lh_lat'    : rots =  [(0,-90),(1,90)  ]\n",
    "    elif view == 'lh_med'    : rots =  [(0,-90),(1,-90) ] \n",
    "    elif view == 'rh_lat'    : rots =  [(0,-90),(1,-90) ]\n",
    "    elif view == 'rh_med'    : rots =  [(0,-90),(1,90)  ]\n",
    "    elif view == 'superior'  : rots =   None\n",
    "    elif view == 'inferior'  : rots =   (1,180)\n",
    "    elif view == 'anterior'  : rots =   (0,-90)\n",
    "    elif view == 'posterior' : rots =  [(0, -90),(1,180)]\n",
    "    elif (type(view) == tuple) or (type(view) == list): rots = view \n",
    "\n",
    "    # (rh_lat is the default 'view' argument because no rotations are \n",
    "    #  for that one; so if no view is specified when the function is called, \n",
    "    #  the 'rh_lat' option is chose here and the surface is shown 'as is'                          \n",
    "                            \n",
    "    # ...apply rotations                          \n",
    "    if rots is None: rotmat = np.eye(3)\n",
    "    else:            rotmat = get_combined_rotation_matrix(rots)\n",
    "    vtx = np.dot(vtx,rotmat)\n",
    "\n",
    "    # 2. Sort out the data\n",
    "                                    \n",
    "    # ...if no data is given, plot a vector of 1s. \n",
    "    #    if using region data, create corresponding surface vector \n",
    "    if data is None: \n",
    "        data = np.ones(vtx.shape[0]) \n",
    "    elif data.shape[0] != vtx.shape[0]: \n",
    "        data = np.array([data[r] for r in rm])\n",
    "    \n",
    "    # ...apply thresholds\n",
    "    if uthr: data *= (data < uthr)\n",
    "    if lthr: data *= (data > lthr)\n",
    "    data *= (np.abs(data) > nz_thr)\n",
    "\n",
    "    # 3. Create the surface triangulation object \n",
    "    x,y,z = vtx.T\n",
    "    tx,ty,tz = vtx[tri].mean(axis=1).T\n",
    "    tr = Triangulation(x,y,tri[np.argsort(tz)])\n",
    "                \n",
    "    # 4. Make the figure \n",
    "    if ax is None: fig, ax = plt.subplots(figsize=figsize)  \n",
    "  \n",
    "    #if shade = 'gouraud': shade_opts['shade'] = \n",
    "    tc = ax.tripcolor(tr, np.squeeze(data), **shade_kwargs)\n",
    "                        \n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    if title is not None: ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_surface_mpl_mv(vtx=None,tri=None,data=None,rm=None,hemi=None,   # Option 1\n",
    "                        vtx_lh=None,tri_lh=None,data_lh=None,rm_lh=None, # Option 2\n",
    "                        vtx_rh=None,tri_rh=None,data_rh=None,rm_rh=None,\n",
    "                        title=None,**kwargs):\n",
    " \n",
    "    if vtx is not None:                                    # Option 1\n",
    "        tri_hemi = hemi[tri].any(axis=1)\n",
    "        tri_lh,tri_rh = tri[tri_hemi==0],tri[tri_hemi==1]\n",
    "    elif vtx_lh is not None:                               # Option 2\n",
    "        vtx = np.vstack([vtx_lh,vtx_rh])\n",
    "        tri = np.vstack([tri_lh,tri_rh+tri_lh.max()+1])\n",
    "\n",
    "    if data_lh is not None:                                # Option 2\n",
    "        data = np.hstack([data_lh,data_rh])\n",
    "    \n",
    "    if rm_lh is not None:                                  # Option 2 \n",
    "        rm = np.hstack([rm_lh,rm_rh + rm_lh.max() + 1])\n",
    "    \n",
    " \n",
    "    # 2. Now do the plots for each view\n",
    "\n",
    "    # (Note: for the single hemispheres we only need lh/rh arrays for the \n",
    "    #  faces (tri); the full vertices, region mapping, and data arrays\n",
    "    #  can be given as arguments, they just won't be shown if they aren't \n",
    "    #  connected by the faces in tri )\n",
    "  \n",
    "    # LH lateral\n",
    "    plot_surface_mpl(vtx,tri_lh,data=data,rm=rm,view='lh_lat',\n",
    "                   ax=subplot(2,3,1),**kwargs)\n",
    "    \n",
    "    # LH medial\n",
    "    plot_surface_mpl(vtx,tri_lh, data=data,rm=rm,view='lh_med',\n",
    "                   ax=subplot(2,3,4),**kwargs)\n",
    "    \n",
    "    # RH lateral\n",
    "    plot_surface_mpl(vtx,tri_rh, data=data,rm=rm,view='rh_lat',\n",
    "                   ax=subplot(2,3,3),**kwargs)\n",
    "    \n",
    "    # RH medial\n",
    "    plot_surface_mpl(vtx,tri_rh, data=data,rm=rm,view='rh_med',\n",
    "                   ax=subplot(2,3,6),**kwargs)\n",
    "    \n",
    "    # Both superior\n",
    "    plot_surface_mpl(vtx,tri, data=data,rm=rm,view='superior',\n",
    "                   ax=subplot(1,3,2),title=title,**kwargs)\n",
    "    \n",
    "    plt.subplots_adjust(left=0.0, right=1.0, bottom=0.0,\n",
    "                      top=1.0, wspace=0, hspace=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_combined_rotation_matrix(rotations):\n",
    "    rotmat = np.eye(3)\n",
    "    \n",
    "    if type(rotations) is tuple: rotations = [rotations] \n",
    "    for r in rotations:\n",
    "        newrot = get_rotation_matrix(r[0],r[1])\n",
    "        rotmat = np.dot(rotmat,newrot)\n",
    "    return rotmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rotation_matrix(rotation_axis, deg):\n",
    "    # (note make deg minus to change from anticlockwise to clockwise rotation)\n",
    "    th = -deg * (pi/180) # convert degrees to radians\n",
    "    \n",
    "    if rotation_axis == 0:\n",
    "        return np.array( [[    1,         0,         0    ],\n",
    "                          [    0,      cos(th),   -sin(th)],\n",
    "                          [    0,      sin(th),    cos(th)]])\n",
    "    elif rotation_axis ==1:\n",
    "        return np.array( [[   cos(th),    0,        sin(th)],\n",
    "                          [    0,         1,          0    ],\n",
    "                          [  -sin(th),    0,        cos(th)]])\n",
    "    elif rotation_axis ==2:\n",
    "        return np.array([[   cos(th),  -sin(th),     0    ],\n",
    "                         [    sin(th),   cos(th),     0   ],\n",
    "                         [     0,         0,          1   ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx_file= '/home/htian/Data/newzip.zip'\n",
    "rm= np.loadtxt('/home/htian/Data/region_mapping.txt')\n",
    "hemi= np.loadtxt('/home/htian/Data/fv91_srfData_20170215/hemispheres.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx_file ='/home/htian/Data/newzip.zip'\n",
    "rm= np.loadtxt('/home/htian/Data/region_mapping.txt').astype(int)\n",
    "hemi= np.loadtxt('/home/htian/Data/fv91_srfData_20170215/hemispheres.txt').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = cortex.Cortex.from_file(source_file = ctx_file)\n",
    "vtx,tri = ctx.vertices,ctx.triangles\n",
    "\n",
    "df_tavg = pd.DataFrame(np.squeeze(tavg_data),index=tavg_time)\n",
    "df_tavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isrh_vtx = np.array([isrh_reg[r] for r in rm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_tavg.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notstim= np.squeeze(tavg_data).copy()\n",
    "notstim[:, stim_node]= 0\n",
    "           \n",
    "m = np.array([(np.abs(notstim[i])).max() for i in range(len(notstim))])\n",
    "for i in range(len(notstim)):\n",
    "    for j in range(len(notstim[1])):\n",
    "        #print i,\n",
    "        notstim[i,j] /= m[i]\n",
    "\n",
    "df_notstim = pd.DataFrame(notstim,index=tavg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vmin_node=df_notstim.ix[t].abs().values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd '/rri_disks/home/akacollja/tvb/brain_plots/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=10)\n",
    "cmap = cm.Reds\n",
    "cmap.set_under(color='w')\n",
    "\n",
    "kws = {'edgecolors': 'k', 'vmin': vmin_node, 'cmap': cmap, \n",
    "       'vmax': 1 , 'alpha': None, 'linewidth': 0.01}\n",
    "\n",
    "\n",
    "ts=[10000.5, 10059.5, 10121.5]#, 10150.5, 10200.5, 10250.5, 10300.5, 100.5, 10400.5, 10450.5]\n",
    "#ts=[10000.5, 10035.5, 10105.5, 10175.5, 10245.5, 10350.5, 10525.5, 10700.5, 10875.5, 11050.5, 11225.5, 11400.5, 11575.5]\n",
    "\n",
    "for t_it,t in enumerate(ts):\n",
    "    data = np.append(df_notstim.ix[t].abs().values, nan)\n",
    " \n",
    "    plot_surface_mpl_mv(vtx=vtx,tri=tri,rm=rm,data=data, figsize=(10,10),\n",
    "                    hemi=hemi ,shade_kwargs=kws) \n",
    "    \n",
    "    f = '/rri_disks/home/akacollja/tvb/brain_plots/braintmp_' + stim_name +'_t%1.1fms.png' %t\n",
    "    plt.savefig(f,bbox_inches='tight')\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=10, figsize= (12,60))\n",
    "for t_it,t in enumerate(ts):\n",
    "    f =  '/rri_disks/home/akacollja/tvb/brain_plots/braintmp_' + stim_name + '_t%1.1fms.png' %t\n",
    "    ax[t_it].imshow(plt.imread(f))\n",
    "    ax[t_it].axis('off')\n",
    "    ax[t_it].set_title('t=%1.1fms' %t,fontsize=9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting eigenvector on brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the data\n",
    "kws = {'edgecolors': 'k', 'vmin': -1, 'cmap': 'coolwarm', 'vmax': 1, 'alpha': None, 'linewidth': 0.01}\n",
    "fig, ax = plt.subplots(ncols=4,nrows=2,figsize=(12,6))\n",
    "for e_it in range(4): \n",
    "    dat = dat_notrans_cov_evecs_155[:,e_it]\n",
    "    \n",
    "    plot_surface_mpl(vtx=vtx,tri=tri,data=dat,rm=rm,ax=ax[0][e_it],\n",
    "                 shade_kwargs=kws,view='rh_lat', title='evec %s' %e_it)\n",
    "    \n",
    "    plot_surface_mpl(vtx=vtx,tri=tri,data=dat,rm=rm,ax=ax[1][e_it],\n",
    "                 shade_kwargs=kws,view='superior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the data with one color\n",
    "\n",
    "kws = {'edgecolors': 'k', 'vmin': 0, 'cmap': 'Reds', #'vmin': 0.4\n",
    "       'vmax': 1, 'alpha': None, 'linewidth': 0.01}\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4,nrows=2,figsize=(12,6))\n",
    "\n",
    "for e_it in range(4):\n",
    "    dat = np.abs(dat_notrans_cov_evecs_155[:,e_it])\n",
    "\n",
    "    plot_surface_mpl(vtx=vtx,tri=tri,data=dat,rm=rm,ax=ax[0][e_it],\n",
    "                     shade_kwargs=kws,view='rh_lat', title='evec %s' %e_it)\n",
    "\n",
    "    plot_surface_mpl(vtx=vtx,tri=tri,data=dat,rm=rm,ax=ax[1][e_it],\n",
    "                     shade_kwargs=kws,view='superior')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all activation times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running all the stimulations you wanted, load in all your files that you saved previously. Here, we will be saving all the activation times in a dataframe to compare activation times of all stimulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "act_times= []\n",
    "act_times.append(np.array(np.loadtxt('S_data.txt')))\n",
    "act_times.append(np.array(np.loadtxt('CA3_data.txt')))\n",
    "act_times.append(np.array(np.loadtxt('CA1_data.txt')))\n",
    "act_times.append(np.array(np.loadtxt('ER_data.txt')))\n",
    "act_times= np.array(act_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_labs= ['S','CA3', 'CA1', 'ER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_activation_spec_all = pd.DataFrame(act_times.T,lab_spec, columns= col_labs) \n",
    "df_activation_spec_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualizing the data with a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= df_activation_spec_all.plot(kind='barh', title='Activation times', figsize=(10,10))\n",
    "\n",
    "ax.set_ylabel('node acronyms')\n",
    "ax.set_xlabel('time activation(ms)')\n",
    "ax.invert_yaxis()\n",
    "ax.set_title('Node Activation Times')\n",
    "ax.set_xlim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
